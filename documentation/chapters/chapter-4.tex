\chapter{Adaptation of Walsh Transformation} \label{ch:walsh transformation}

\section{Walsh Transorm}
A time series, f(t), in terms of a series of Walsh funcitons W(n,t) \cite{Beauchamp1975}, viz.
\[
f(t) = a_{0} W_{0,t} + \sum_{n=1}^{N-1} a_n W_{n,t}
\]
where $n$ is an ordering number, $N$ is number of terms used in Walsh series to express time series and
\[
\frac{a_0}{2} = \frac{1}{T} \int\limits_0^T f(t) W_{n,t} dt
\]
\[
a_n = \frac{1}{T} \int\limits_0^T f(t) W_{n,t} dt
\]

Finite discrete Walsh transform pair on N sampling points, $x_t$, can be expressed as \cite{Beauchamp1975} 
\begin{equation}
\label{WalshT}
X_n = \frac{1}{N} \sum_{t=0}^{N-1} x_t W_{n,t}
\end{equation}
\[
n = 0, 1, 2...N-1
\]
and
\[
x_t = \sum_{n=0}^{N-1} X_n W_{n,t}
\]
\[
t = 0, 1, 2...N-1
\]

The Walsh function series $W_{n,t}$ can be obtained using Walsh matrix also known as Hadamard matrix of order N. 
Walsh matrix or Hadamard matrix is a square matrix of order N whose coefficients comprise only +1 and -1 and where its rows 
(and columns) are orthogonal to one another. 
The Walsh matrix is defined by
\[
W_{n,t} = N^{-1/2} (-1)^{n \cdot t}
\]
where $N^{-1/2}$ is normalization factor and $n \cdot t$ is bitwise dot product of binary representation of number n and t.

The matrix is symmetric, i.e.,
\[
W_{n,t} = W_{n,t}
\]
and it has entries satisfying
\[
W_{n, t \oplus k} = N^{1/2} W_{n, t} W_{n, k}
\]

The practical importance of this symmetry is that the transform and inverse represent same mathematical operation, hence simplifying the derivation and application of the transform. With the normalized form, \textit{Walsh matrix} is its own inverse, i.e.,
\[
W = W^{-1}
\]

In the matrix form, given vector $w$ and matrix $A$, let $\widehat{w}$ and
$\widehat{A}$ denote the Walsh transform of $w$ and $A$ respectively. Then $\widehat{w} = Ww$ and
$\widehat{A} = WAW$. If $w$ is a row vector, then $w$ in its Walsh basis $\widehat{w}$ represents $wW$.

\section{Walsh Transform Adaptation}
The Walsh transform has spectacular ability to unravel the intricacies of mixing. And that is why we adapt Walsh transform methods for computing evolutionary trajectories, which have already been established for Vose's haploid model \cite{Vose1999}. Adaptation of Walsh transformation efficiently models infinite diploid population evolution. This adaptation of Walsh transormation helps in making feasible comparisons between finite and infinte diploid population short-term evolutionary behavior.
Recalling evolution equation (\ref{model4}), without selection, specialized to Vose's infinite population model expressed in mixing matrix's term,
\[
p_g^\prime \; = \; (\sigma_g \nudge p)^T M \, (\sigma_g \nudge p)
\]
where the permutation matrix $\sigma_g$ is defined by component
equations
\[
(\sigma_g)_{u,v} \; = \; [\nudge u+v = g\nudge ]
\]

In our model, the Walsh matrix $W$
is defined by component equations
\[
W_{u,v} \; = \; 2^{-\ell/2} (-1)^{u^T v}
\]
where the subscripts \nudge u, \nudge v (which belong to $\mathcal{R}$) on the left hand side are interpreted on the right hand side as column vectors in $\mathbb{R}^{\ell}$.
Columns of $W$ form the orthonormal basis --- the
{\em Walsh basis\/} --- which simultaneously diagonalizes the
$\sigma_g$.

A change of basis which simultaneously diagonalizes the $\sigma_g$
unravels the evolution equation (\ref{model4}).  
Expressed in the Walsh basis (see \cite{Vose1999}), the mixing matrix
takes the form
\begin{equation}
\label{Mhat}
\widehat{M}_{u,v} \; = \; 2^{\,\ell-1} \,[\nudge u \nudge v = {\bf
    0}\nudge]\, \widehat{\mu}_u \nudge \widehat{\mu}_v \!  \sum_{k
  \nudge \in \nudge \overline{u+v} \nudge \mathcal{R}} \chi_{k + u} +
\chi_{k + v}
\end{equation}
and equation (\ref{model4}) takes the form
\begin{equation}
\label{model5}
\widehat{p}_g^{\,\,\prime} \; = \; 2^{\,\ell/2} \sum_{i \nudge \in \nudge g \mathcal{R}}
\widehat{p}_i \, \nudge \widehat{p}_{i+g} \,\widehat{M}_{i,i+g}
\end{equation}
where $g \mathcal{R} = \{g \nudge i \, | \, i \in \mathcal{R} \}$ (for
any $g \in \mathcal{R}$).

The mapping from generation $n$ to generation $n+1$, determined in
natural coordinates by equation (\ref{model3}) in terms of the
transmission function (\ref{Mg}), and given in Walsh coordinates by
equation (\ref{model5}) in terms of the mixing matrix (\ref{Mhat}), is
Markovian; the next state $p^\prime$ depends only upon the current
state $p$.  Let $\mathcal{M}$ represent the mixing transformation,
\begin{equation} \label{mixing_transformation}
p^\prime \; = \; \mathcal{M}(p)
\end{equation}
and let $\mathcal{M}^n(p)$ denote the $n$-fold composition of
$\mathcal{M}$ with itself; thus generation $n$ is described by
\[
p^n \; = \; \mathcal{M}^n(p^0)
\]
where $p^0$ represents the initial population.  We have little to say
about the matrix of the Markov chain corresponding to the mixing
transformation $\mathcal{M}$, because it is uncountable; each state is
a distribution vector $p$ describing a population. However, that is
not an obstacle to computing evolutionary trajectories;
(\ref{mixing_transformation}) can be computed in Walsh coordinates
relatively efficiently via (\ref{Mhat}) and (\ref{model5}).

\section{Fast Walsh Transform}
However, computation of discrete Walsh transform given by equation (\ref{WalshT}) takes $N^2$ operations (addition or subtraction).
An algorithm using matrix factorization techniques is found to perform transformation in $N \log_2 N$ operations.
This algorithm in fast Walsh transform (FWT). 
Shanks \cite{Shanks1969} described FWT algorithm which is analogous to Cooley-Tukey \cite{CooleyTukey1965} algorithm for fast Fourier transformation. Shanks assumed walsh function to be periodic with period $N$, where $N$ is an integral power of 2. So a complete orthogonal set will have $N$ function $W_{m,n}$ where $m = 0, 1, 2,.., N-1$ and $n = 0, 1, 2,.., N-1$. The first two discrete walsh functions are defined as 
\begin{equation}
\label{FWT1}
W_{0,n} = 1    \text{ for $n = 0, 1, 2,.., N-1$}
\end{equation}
\begin{equation}
\label{FWT2}
W_{1,n} = \begin{cases}
    1 & \text{for $n = 0, 1, 2,.., (N/2)-1$}.\\
    -1 & \text{for $n = N/2, (N/2)+1,.., N-1$}.
  \end{cases}
\end{equation}

Remainder of set can be generated by using multiplicative iterative equation (\ref{FWTIterative}): 
\begin{equation}
\label{FWT3}
W_{m,n} = W_{[m/2],2n}\cdot W_{m-2[m/2], n}
\end{equation}
where $[m/2]$ indicates the integer part of $m/2$.

The discrete Walsh functions as defined here are symmetric with respect to the argument (m, n). That is,
\[
W_{m, n} = W_{n,m}.
\]

For real array of length $N$, Walsh transform can be defined as 
\begin{equation}
\label{FWT4}
F(m) = \sum\limits_{n=0}^{N-1} f(n) W_{m,n}, \text{where $m = 0, 1, 2,.., N-1$}
\end{equation}

Similarly, for inverse transform is
\begin{equation}
\label{FWT5}
f(n) = \frac{1}{N}\sum\limits_{m=0}^{N-1} F(m) W_{m,n}, \text{where $n = 0, 1, 2,.., N-1$}
\end{equation}

Since walsh functions $W_{m,n}$ have values either $1$ or $-1$, computation of (\ref{FWT4}) and (\ref{FWT5}) does not require multiplication.
Shanks \cite{Shanks1969} derived using (\ref{FWT3}) an algorithm analogous to Cooley-Tukey algorithm that will require $N \log_2 N$ summations to compute complete Walsh transform. \linebreak
For $N = 8$ (which can be extended to general case),\linebreak
indices in (\FWT4) can be replaced with a set which can only have values 0 and 1. That is, 
\begin{equation}
\label{FWT6}
m = 4j_2 + 2j_1 + j_0, \text{$j_2, j_1, j_0 = 0$ or $1$}
\end{equation}
\begin{equation}
\label{FWT7}
n = 4k_2 + 2k_1 + k_0, \text{$k_2, k_1, k_0 = 0$ or $1$}
\end{equation}
Using these new notations, $W_{m,n}$ becomes $W(j_2,j_1,j_0;k_2,k_1,k_0)$. So (\ref{FWT4}) becomes 
\begin{equation}
\label{FWT8}
F(j_2,j_1,j_0) = \sum\limits_{k0 = 0}^1 \sum\limits_{k1 = 0}^1 \sum\limits_{k2 = 0}^1 f(k2,k1,k0) \cdot W(j_2,j_1,j_0;k_2,k_1,k_0)
\end{equation}

Here, $j_2,j_1,j_0$ is binary representation of $m$. So, dividing $m$ by $2$ equivalents to shifting binary representation of $m$ by $1$ to the right and dropping fractional bit. That is, if 
\[
m \leftrightarrow j_2j_1j_0
\]
then,
\[
[m/2] \leftrightarrow 0j_2j_1.
\]

Similarly, if
\[
n \leftrightarrow k_2k_1k_0
\]
then,
\[
2n \leftrightarrow k_2k_1k_00.
\]
An 8-length Walsh function is periodic with period 8. Thus any index (such as $2n$) can be evaluated modulo 8. This is equivalent to deleting any bits above the third bit and we have 
\[
2n(modulo 8) \leftrightarrow k_1K_00
\]

Using these indices, (\ref{FWT3} becomes
\begin{equation}
\label{FWT9}
W(j_2,j_1,j_0;k_2,k_1,k_0) = W(0,j_2,j_1;k_1,k_0,0) \cdot W(0,0,j_0;k_2,k_1,k_0).
\end{equation}

$j_0$ can only be 0 or 1, so $W(0,0,j_0; k_2, k_1, k_0)$ represents either $W_{0,n} or W_{1,n}$. The function $W_{0,n}$ is 1 for all $n$. And function $W_{1,n}$ is 1 if $0<n<4$ and $W_{1,n}$ is -1.0 if $4<n<7$. Thus, from (\ref{FWT7}), $W_{1,n}$ is +1.0 if $k_2=0$ and $W_{1,n}$ is -1.0 if $k_2 = 1$. Therefore, 
\begin{equation}
\label{FWT10}
W(0,0,j_0;k_2,k_1,k_0) = (-1)^{j_0k_2}.
\end{equation}

(\ref{FWT9}) can be used to factor $W(0,j_2,j_1;k_1,k_0,0)$ to get
\begin{equation}
\label{FWT11}
W(0,j_2,j_1;k_1,k_0,0) = W(0,0,j_2;k_0,0,0) \cdot W(0,0,j_1;k_1,k_0,0).
\end{equation}

Using (\ref{FWT10}) and (\ref{FWT11}) in (\ref{FWT9}), we get completely factored expression for the 8-length Walsh function
\begin{equation}
\label{FWT12}
W(j_2,j_1,j_0;k_2,k_1,k_0) = (-1)^{j_2k_0}(-1)_{j_1k_1}(-1)^{j_0k_2}.
\end{equation}

Using (\ref{FWT12}), (\ref{FWT8}) becomes 
\begin{equation}
\label{FWT13}
F(j_2,j_1,j_0;k_2,k_1,k_0) = \sum\limits_{k_0=0}^1 (-1)^{j_2k_0} \sum\limits_{k_1=0}^1 (-1)^{j_1k_1}  \cdot  \sum\limits_{k_2=0}^1 (-1)^{j_0k_2} f(k_2,k_1,k_0).
\end{equation}

We can define
\begin{equation}
\label{FWT14}
A_1(j_0,k_1,k_0) = \sum\limits_{k_2=0}^1 (-1)^{j_0k_2} f(k_2,k_1,k_0)
\end{equation}

Array $A_1$ is first set of intermediate calculations to compute $F(m)$. $A_2(j_0,j_1,k_0)$ can be computed from $A_1$ and final result $A_3(j_0,j_1,j_2)$ from $A_2$ for 8-length Walsh transform. However, the order of the values will be in the bit reversed form. It can be generalized for the case $N = 2^p$ by defining intermediate Walsh transform arrays as
\begin{equation}
\label{FWT15}
A_l(j_O,j_1,...,j_{l-1},k_{p-l-1},...,k_0) = \sum\limits_{k_{p-l}=0}^1 A_{l-1}(j_0,j_1,...,j_{l-2},k_{p-l},k_{p-l-1},...,k_0) \cdot (-1)^{j_{l-1}k_{p-l}}
\end{equation}
where $l = 1,2,...,p$ and 
\begin{equation}
\label{FWT16}
A_0(k_{p-1},k_{p-2},...,k_0) = f(k_{p-1},k_{p-2},...,k_0).
\end{equation}

The general equation for the $M= 2^P$-length discrete Walsh function is 
\[
W(j_{p-1},j_{p-2},...,j_0;k_{p-1},k_{p-2},...,k_0) = \prod_{i=0}^{p-1} (-1)^{j_{p-1-i}k_i}.
\]


\section{Simplification} 
The haploid case simplified by equations (\ref{Mhat}) and (\ref{model5})
are the consequence of specializing to Vose's infinite population model and computing in the Walsh basis. Time switching between the standard basis and the Walsh basis is negligible; the fast Walsh transform (in dimension $n$) has complexity $n \nudge \log n$ \cite{Shanks1969}.

Only one mixing matrix as opposed to $2^\ell$ matrices is needed to compute the next generation; evolution equation (\ref{model5}) references the same matrix for every $g$, whereas evolution equation (\ref{model3}) depends upon a different matrix $M_g$ for each choice of $g$. The matrix is computed by a single sum as opposed to a triple sum; compare equation (\ref{Mhat}) with equation (\ref{transmission}).  Also, the relevant quadratic form is computed with a single sum as opposed to a double sum; computing via (\ref{model5}) is linear time in the size of $g \mathcal{R}$ (for each $g$) as opposed to the quadratic time computation (for each $g$) represented by equation (\ref{model3}).

From a computational standpoint, the best-case scenario is where
recomputation of the matrices mentioned in the previous paragraph is
obviated by sufficient memory.  The reduction from $2^\ell$ matrices
to one matrix helps significantly in that regard. To demonstrate these advantages in concrete terms, consider computing
with genomes of length $\ell \in \{4,6,8,10,12,14\}$.  The fact that
only the mixing matrix need be involved is significant; using $2^{14}$
matrices each of which contains $2^{14} \times 2^{14}$ entries of type
\verb@double@ requires $32$ terabytes, whereas the mixing matrix at
$2$ gigabytes fits easily within the memory of a laptop.


